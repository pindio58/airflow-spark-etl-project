# --- Stage 1: Main PySpark image ---
FROM apache/spark-py:latest

USER root

# --- Set environment variables ---
ENV SPARK_HOME=/opt/spark
ENV PATH=$SPARK_HOME/bin:$SPARK_HOME/sbin:/usr/local/bin:$PATH

# --- Verify installations (Java + Python + Spark) ---
RUN java -version && python3 --version && spark-submit --version

# --- Add Hadoop AWS + AWS SDK jars for S3 support ---
ARG HADOOP_VERSION=3.3.4
ENV HADOOP_VERSION=${HADOOP_VERSION}
WORKDIR ${SPARK_HOME}/jars

RUN set -eux; \
    echo "Downloading Hadoop + AWS JARs into $(pwd)"; \
    wget -q https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/${HADOOP_VERSION}/hadoop-aws-${HADOOP_VERSION}.jar && \
    wget -q https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar && \
    ls -lh *aws*jar

# --- Install useful Python packages ---
RUN pip install --no-cache-dir \
    findspark \
    requests \
    python-dotenv
    # pandas \
    # pyarrow \
    # boto3

# --- Working directory for Spark jobs ---
WORKDIR /opt/spark/work-dir

# --- Drop to non-root user (good practice) ---
# USER 1001
